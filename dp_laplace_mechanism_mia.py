# -*- coding: utf-8 -*-
"""DP_Laplace_Mechanism_MIA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ACyREuUDJwigzwPYYDEr0k50K8xSzcei
"""

import numpy as np
import pandas as pd
import sklearn as sk
import matplotlib.pyplot as plt

df = pd.read_csv('https://www.cis.upenn.edu/~mkearns/teaching/EADSpring24/california_housing_train.csv')
df

num_cols = df.shape[1]
num_rows = 3
num_bins = 100

fig, axs = plt.subplots(num_rows, num_cols // num_rows, figsize=(15, 10))
fig.tight_layout(pad=4.0)

for i, col in enumerate(df.columns):
  ax = axs[i // num_rows, i % num_rows]
  ax.hist(df[col], bins=num_bins, alpha=0.7, color='blue', edgecolor='black')

  mean_val = np.mean(df[col])
  median_val = np.median(df[col])
  max_val = np.max(df[col])
  min_val = np.min(df[col])
  std_dev = np.std(df[col])

  ax.axvline(mean_val, color='red', linestyle='dashed', linewidth=1, label='Mean')
  ax.axvline(median_val, color='green', linestyle='dashed', linewidth=1, label='Median')
  ax.axvline(max_val, color='black', linestyle='dashed', linewidth=1, label='Max')
  ax.axvline(min_val, color='orange', linestyle='dashed', linewidth=1, label='Min')

  ax.errorbar(mean_val, 100, xerr=std_dev, fmt='o', color='red')

  ax.set_title(col)

handles, labels = ax.get_legend_handles_labels()
fig.legend(handles, labels, loc='upper right')

plt.show()

normalized_df = np.abs((df - df.min())) / np.abs((df.max() - df.min()))
normalized_df

def laplace_mechanism(true_value, epsilon, sensitivity, num_samples):
  b = sensitivity / epsilon

  laplace_noise = np.random.laplace(scale=b, size=num_samples)

  return true_value + laplace_noise

def sensitivity(func, n):
  if func == 'Mean':
      return 1 / n
  elif func == 'Std':
      return 1 / np.sqrt(n)
  elif func == 'Max' or func == 'Min' or func == 'Median':
      return 1

def compute_accuracy(column_data, func, func_name, epsilon, num_points, num_samples):
  true_value = func(column_data)
  sens = sensitivity(func_name, num_points)
  samples = laplace_mechanism(true_value, epsilon, sens, num_samples)
  return np.abs(samples - true_value).mean()

functions = {
    'Mean': lambda x: np.mean(x),
    'Median': lambda x: np.median(x),
    'Max': lambda x: np.max(x),
    'Min': lambda x: np.min(x),
    'Std': lambda x: np.std(x)
}

columns = normalized_df.columns
epsilons = np.arange(0.1, 5.1, 0.1)
num_points_range = np.arange(100, len(normalized_df), 2500)
num_samples = 1000

plt.figure(figsize=(20, 30))

plot_index = 1
for col in columns:
    column_data = normalized_df[col]
    for (func_name, func) in functions.items():
        plt.subplot(len(columns), len(functions), plot_index)
        plt.title(f"{col} - {func_name}")
        for num_points in num_points_range:
            prefix_col_data = column_data[:num_points]
            errors = [compute_accuracy(prefix_col_data, func, func_name, epsilon, num_points, num_samples) for epsilon in epsilons]
            plt.plot(epsilons, errors, label=f'n={num_points}', color=plt.cm.viridis(num_points / len(normalized_df)))
        plt.xlabel('Epsilon')
        plt.ylabel('Accuracy')
        plt.legend()
        plt.grid(True)
        plot_index += 1

plt.tight_layout()
plt.show()

"""The results show that at $ϵ$ = 0.01 (near perfect privacy), error is generally much higher and slowly decreases as $ϵ$ increases. This is not unexpected since differential privacy should lower the accuracy of a model. This difference is much less pronounced on some functions compared to others.

Some columns varied extremely wildly from the general idea that lower samples had higher error on average. The largest differences come with two categories: Mean/Std vs. Median/Max/Min. The error is much smaller with Mean/Std than with Median/Max/Min. This is likely because of the sensitivity calculation where Mean/Std are smaller at $\frac{1}{n}$ and $\frac{1}{\sqrt{n}}$ while Median/Max/Min are 1. Therefore, it is reasonable to conclude that Mean/Std are in the sweet spot for differential privacy.
"""

normalized_100_df = normalized_df * 100
normalized_100_df

def MIA(f, x, x_prime, y):
  x_result = f(x)
  x_prime_result = f(x_prime)
  if np.sum(abs(y - x_result)) <= np.sum(abs(y - x_prime_result)):
    return 0
  return 1

def check_correct(column_data, func, func_name, epsilon, num_points, num_samples):
  random_index = np.random.randint(len(column_data))
  x_prime = np.delete(column_data, random_index)

  rng = np.random.default_rng()
  choice = -1

  if rng.random() > 0.5:
    true_value = func(column_data)
    choice = 0
  else:
    true_value = func(x_prime)
    choice = 1

  sens = sensitivity(func_name, num_points)
  samples = laplace_mechanism(true_value, epsilon, sens, num_samples)
  return MIA(func, column_data, x_prime, samples) == choice

epsilons = np.geomspace(1e-2, 1e8, 100)
colors = [0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 0.999]

num_points_range = [3, 5, 10, 50, 100, 1000, 10000, 17000]
num_samples = 1000

fig, axes = plt.subplots(len(columns), len(functions), figsize=(20, 30))

for i, col in enumerate(columns):
    column_data = normalized_df[col]
    for j, (func_name, func) in enumerate(functions.items()):
        ax = axes[i, j]
        ax.set_title(f"{col} - {func_name}")
        ax.set_xscale("log")
        for k, num_points in enumerate(num_points_range):
            prefix_col_data = column_data[:num_points]
            error = []
            for epsilon in epsilons:
              results = [check_correct(prefix_col_data, func, func_name, epsilon, num_points, num_samples) for _ in range(500)]
              error.append(np.mean(results))
            ax.plot(epsilons, error, label=f'n={num_points}', color=plt.cm.viridis(colors[k]))
        ax.set_xlabel('Epsilon')
        ax.set_ylabel('Accuracy')
        ax.legend()
        ax.grid(True)

plt.tight_layout()
plt.show()

"""Generally off the plots, it seems like as epsilon increases (very low DP), it becomes easier to guess which dataset $y$ was taken from. This is intuitive because the data is basically untouched.
MIA success rate generally decreases with larger prefixes where the "blue-r" colors (lower prefix sizes) reach perfect accuracy with a lower epsilon value, but there are some exceptions such as max/min where the blue colors separated from pure luck (50%), but could not hit 100% as well as some plots in mean.

Earlier, I concluded that Mean/Std were at the sweet spot for DP and it seems to hold since those graphs clearly go from 0.5 to 1 with higher epsilon regardless of prefix length while Median/Max/Min show poor MIA performance especially at higher prefix lengths.
"""